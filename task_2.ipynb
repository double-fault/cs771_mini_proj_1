{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read emoticon dataset\n",
    "train_emoticon_df = pd.read_csv(\"datasets/train/train_emoticon.csv\")\n",
    "train_emoticon_X = train_emoticon_df['input_emoticon'].tolist()\n",
    "train_emoticon_Y = train_emoticon_df['label'].tolist()\n",
    "\n",
    "test_emoticon_X = pd.read_csv(\"datasets/test/test_emoticon.csv\")['input_emoticon'].tolist()\n",
    "\n",
    "\n",
    "# read text sequence dataset\n",
    "train_seq_df = pd.read_csv(\"datasets/train/train_text_seq.csv\")\n",
    "train_seq_X = train_seq_df['input_str'].tolist()\n",
    "train_seq_Y = train_seq_df['label'].tolist()\n",
    "\n",
    "test_seq_X = pd.read_csv(\"datasets/test/test_text_seq.csv\")['input_str'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "# read feature dataset\n",
    "train_feat = np.load(\"datasets/train/train_feature.npz\", allow_pickle=True)\n",
    "train_feat_X = train_feat['features']\n",
    "train_feat_Y = train_feat['label']\n",
    "\n",
    "test_feat_X = np.load(\"datasets/test/test_feature.npz\", allow_pickle=True)['features']\n",
    "\n",
    "# Read validation emoticon dataset\n",
    "valid_emoticon_df = pd.read_csv(\"datasets/valid/valid_emoticon.csv\")\n",
    "valid_emoticon_X = valid_emoticon_df['input_emoticon'].tolist()\n",
    "valid_emoticon_Y = valid_emoticon_df['label'].tolist()\n",
    "\n",
    "# Read validation text sequence dataset\n",
    "valid_seq_df = pd.read_csv(\"datasets/valid/valid_text_seq.csv\")\n",
    "valid_seq_X = valid_seq_df['input_str'].tolist()\n",
    "valid_seq_Y = valid_seq_df['label'].tolist()\n",
    "\n",
    "# Read validation feature dataset\n",
    "valid_feat = np.load(\"datasets/valid/valid_feature.npz\", allow_pickle=True)\n",
    "valid_feat_X = valid_feat['features']\n",
    "valid_feat_Y = valid_feat['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Training Data Shape: (7080, 13, 50)\n",
      "Transformed Validation Data Shape: (489, 13, 50)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Assuming train_feat_X and valid_feat_X are defined\n",
    "n_components = 50  # Number of components you want to preserve\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Reshape the training data to (7080 * 13, 768)\n",
    "train_feat_X_reshaped = train_feat_X.reshape(-1, 768)  # Shape will be (7080*13, 768)\n",
    "\n",
    "# Standardize the reshaped training data\n",
    "train_feat_X_scaled = scaler.fit_transform(train_feat_X_reshaped)\n",
    "\n",
    "# Apply PCA\n",
    "train_X_pca = pca.fit_transform(train_feat_X_scaled)\n",
    "\n",
    "# Reshape back to (7080, 13, 50)\n",
    "X_pca = train_X_pca.reshape(7080, 13, n_components)\n",
    "\n",
    "# Now handle the validation data similarly\n",
    "valid_feat_X_reshaped = valid_feat_X.reshape(-1, 768)  # Shape will be (len(valid_feat_X)*13, 768)\n",
    "valid_feat_X_scaled = scaler.transform(valid_feat_X_reshaped)\n",
    "\n",
    "# Apply PCA on the validation data\n",
    "valid_X_pca = pca.transform(valid_feat_X_scaled)\n",
    "\n",
    "# Reshape back to (len(valid_feat_X), 13, 50)\n",
    "valid_X_pca = valid_X_pca.reshape(len(valid_feat_X), 13, n_components)\n",
    "\n",
    "# Now X_pca and valid_X_pca contain the PCA-reduced data\n",
    "print(\"Transformed Training Data Shape:\", X_pca.shape)  # Should be (7080, 13, 50)\n",
    "print(\"Transformed Validation Data Shape:\", valid_X_pca.shape)  # Should be (len(valid_feat_X), 13, 50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# pca = PCA(n_components= 50)\n",
    "\n",
    "# X_train_feat = train_feat_X.reshape(train_feat_X.shape[0], -1)\n",
    "# X_train_feat_scaled = scaler.fit_transform(X_train_feat)\n",
    "# X_train_feat = pca.fit_transform(X_train_feat_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = X_pca.reshape(X_pca.shape[0], -1)\n",
    "valid_ds2 = valid_X_pca.reshape(valid_X_pca.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emo_X = train_emoticon_df['input_emoticon'].tolist()\n",
    "train_emo_X = np.array([[ord(x) for x in e] for e in train_emo_X])\n",
    "train_emo_Y = train_emoticon_df['label'].tolist()\n",
    "\n",
    "valid_emo_X = valid_emoticon_df['input_emoticon'].tolist()\n",
    "valid_emo_X = np.array([[ord(x)  for x in e] for e in valid_emo_X])\n",
    "valid_emo_Y = valid_emoticon_df['label'].tolist()\n",
    "\n",
    "emoji_encoder = LabelEncoder()\n",
    "\n",
    "# Flatten the list of emojis and fit the label encoder\n",
    "# Since train_emo_X is 2D (13, 786), we need to flatten it for unique emojis\n",
    "flat_train_emojis = [emoji for sublist in train_emo_X for emoji in sublist]\n",
    "emoji_encoder.fit(flat_train_emojis)\n",
    "\n",
    "# Transform the training emojis to their corresponding numerical labels\n",
    "train_emo_X_encoded = np.array([[emoji_encoder.transform([x])[0] for x in e] for e in train_emo_X])\n",
    "\n",
    "# Transform the validation emojis using the same encoder\n",
    "valid_emo_X_encoded = np.array([[emoji_encoder.transform([x])[0] for x in e] for e in valid_emo_X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = np.array([[int(char) for char in sequence] for sequence in train_seq_X])\n",
    "X_valid_seq = np.array([[int(char) for char in sequence] for sequence in valid_seq_X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = np.concatenate((np.array(train_emo_X_encoded), ds2 , np.array(X_train_seq)), axis=1)\n",
    "X_valid_final = np.concatenate((np.array(valid_emo_X_encoded), valid_ds2 , np.array(X_valid_seq)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9693251533742331\n"
     ]
    }
   ],
   "source": [
    "model =make_pipeline(StandardScaler(), LogisticRegression(max_iter=200))\n",
    "\n",
    "model.fit(X_train_final, train_seq_Y)\n",
    "\n",
    "print(model.score(X_valid_final, valid_seq_Y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "771_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
